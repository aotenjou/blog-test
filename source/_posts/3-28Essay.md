---
title: 3.28Essay
date: 2025-03-28 14:28:06
tags:
- 随记
---
 <!-- more -->

## MCP： Model Context Protocol

#### bg

应用程序和 AI 模型之间交换上下文信息的方式。这使得开发者能够以**一致的方式将各种数据源、工具和功能连接到 AI 模型**（一个中间协议层）。
MCP 就是以更标准的方式让 LLM Chat 使用不同工具。

![](https://pic1.imgdb.cn/item/67e646f10ba3d5a1d7e58acd.png)

*更结构化的上下文信息*

基于**prompt engineering**：tool的结构化描述和example

最早：手工实现prompt协作

后来引入`function call`，允许模型在需要时**掉用于定义函数来获取数据、执行操作**，局限性在于**平台依赖性强**，`function call API`差异大

MCP最大优势：

- 生态：现成插件。
- 统一性：支持很多模型自由切换。
- 数据安全：自行设计接口，不必上传。

#### how 2 use?
[使用doc](https://modelcontextprotocol.io/quickstart/user)

#### 架构

三个核心组件：Host,client,server。

- host：接受用户请求，与模型交互的接口。

- client：模型决定需要访问文件系统时，激活host内置的MCP client.该client与适当MCP server建立连接。

- server：MCP server会被调用。执行实际系统层面操作。

提出问题后的执行步骤

1. 由 LLM 确定使用哪些 MCP Server（**通过prompt确定有哪些工具**）。
2. 执行对应的 MCP Server 并对执行结果进行重新处理。

*工具具体使用描述：文本形式传递。调用：json*


tool的描述与`input schema`基本上由用户自定义。


## Do We Need Zero Training Loss After Achieving Zero Training Error? （ICML，2020）
一行代码一篇a.

损失函数变为：$\tilde{L} (\theta)=|L(\theta)-b|+b$

效果：**二次下降**。损失函数达到b之后，训练流程大概就是在交替执行梯度下降和梯度上升。

相当于一开始就加入梯度惩罚。达到推动参数往更平稳的区域走。

***


## RAG通识

检索增强生成（Retrieval Augmented Generation）

简单来讲，RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答。

因此，可以将RAG的核心理解为“检索+生成”。
- 前者主要是利用向量数据库的高效存储和检索能力，召回目标知识；
- 后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案


**完整的RAG应用流程主要包含两个阶段**：

    数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库
    应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案

#### 数据准备阶段
- 数据提取：
    - 数据加载：包括多格式数据加载、不同数据源获取等，根据数据自身情况，将数据处理为同一个范式。
    - 数据处理：包括数据过滤、压缩、格式化等。
    - 元数据获取：提取数据中关键信息，例如文件名、Title、时间等 。
- 文本分割：
    - 文本分割主要考虑两个因素：1）embedding模型的Tokens限制情况；2）语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下：
    - 句分割：以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。
    - 固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度（例如256/512个tokens），这种切分方式会损失很多语义信息，一般通过在头尾增加一定冗余量来缓解。
- 向量化（embedding）：
    文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果。常见：ChatGPT,ERNIE,M3E,BGE。
- 数据入库
    - 数据向量化后构建索引，并写入数据库。适用于RAG场景的数据库包括：FAISS、Chromadb、ES、milvus等。

#### 应用阶段
- 用户提问：
    - 根据用户的提问，通过高效的检索方法，召回与提问最相关的知识，并融入Prompt。
- 数据检索：
    - 相似性检索、全文检索等，可以选择多种检索方式融合，提升召回率。
    - 相似性检索：即计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括：余弦相似性、欧氏距离、曼哈顿距离等。
    - 全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。
- 注入Prompt
    - Prompt的设计只有方法、没有语法，比较依赖于个人经验，在实际应用过程中，往往需要根据大模型的实际输出进行针对性的Prompt调优。
***

## MOE（混合专家模型）通识

能够在远少于*稠密模型*所需的计算资源下进行有效的预训练。

主要由两个关键部分组成:
1. 稀疏 MoE 层: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。
2. 门控网络或路由: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。

![](https://pic1.imgdb.cn/item/67e661890ba3d5a1d7e598e0.png)

总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。

**挑战**：

1. 微调泛化能力不足，容易过拟合。

2. 大量参数内存加载问题。

#### bg

与集成学习方法相似，旨在为由多个单独网络组成的系统建立监管机制。

每个（“专家”）：处理训练样本的不同子集，专注于输入空间的特定区域。

门控网络：选择特定专家。

后续：

- 组件专家：moe潜入多层网络的某一层。

- 条件计算：基于动态*令牌*激活或停用网络组建。

#### 什么是稀疏性？

*条件计算*。
传统的稠密模型中，所有的参数都会对所有输入数据进行处理。
相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。
这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。
条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。

*如果令牌都发到少数几个受欢迎的专家就会降低训练效率。*
批量分配：可学习门控网络。**添加噪声：负载均衡。**

**辅助损失**，鼓励给予所有专家相同重要性。
**随机路由**，在top-2设置中，始终选择排名最高的专家，*但第二个是根据其权重比例随机选择。*
**专家容量**，一个专家能处理令牌数量的*阈值*。如果两个专家都达到上限，令牌溢出，通过残差连接传递到下一层，或某些情况下完全丢弃。所有张量的形状在编译时是静态确定的，我们无法提前知道多少令牌会分配给每个专家，因此需要一个固定的容量因子。

#### Switch Transformers

与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:
1. 减少门控网络 (路由) 计算负担
2. 每个专家的批量大小至少可以减半
3. 降低通信成本
4. 保持模型质量

![](https://pic1.imgdb.cn/item/67e66f070ba3d5a1d7e59ed9.png)










