---
title: 3.21,3.22
date: 2025-03-22 19:27:53
tags:
---
 <!-- more -->
读论文。
## 1.PVLDB：memformer

**idea**：分块循环图学习，全局attention,交替记忆增强器
**knowledge**：
1. 正则化：**防止过拟合！** 显式正则化/隐式正则化，机器学习基础内容，回顾一下。
显：
- L1（lasso）正则化：向损失函数添加权重**绝对值之和**作为惩罚，迫使部分权重趋近于0。
- L2（ridge）正则化：向损失函数添加权重**平方之和**作为惩罚，限制权重幅度使其平滑。
- Dropout：训练时，关闭部分神经元（概率p），迫使网络学习冗余表示。测试时激活所有，权重乘（1-p）保持期望值。
- 弹性网络：L1L2搞在一起。
隐：
- 数据增强：训练数据加噪/旋转，增加数据多样性。
- 早停：验证集性能不再提升时终止训练，防止过拟合。
- 批量归一化：每层输入标准化，缓解内部协变量偏移。

2. 子序列建模：
- 通道独立/线性模型：每个变量单独并行处理，隐式参数共享/处理后数据整合。
- transformer类：注意力机制，异常值敏感。

3. 交替记忆增强
- 局部增强器：局部动态特征，细粒度关联建模。
- 全局增强器：全局模式，增强抗干扰。
- 交替训练机制。

***

## 2.OSDI 2020：learned index 4 lsm
**idea**：LSM树结合学习索引，贪婪线性回归学习。
**knowledge**
1. 学习索引
- 基于ml.
- 查询一个key时，系统使用该索引/该函数预测出查询key对应位置。
- 不直接构建数据结构节省空间，提升查找性能节省时间。
- 现有大多数在B树，所以用在lsm树上。

2. 理论“矛盾”
- 学习索引主要针对only-read,lsm针对write.
- 然而*insight*在于，虽然write改变了LSM,但树大多数部分不可变。——> 学习一个预测KV位置的值的函数只需要完成一次，只要**不可变的数据还在**就能使用它。

3. guideline after [Wisckey](https://zhuanlan.zhihu.com/p/486811231)
- 学习LSM中，稳定的低级别有效；查找先搜索更高级别，学习更高级别也有好处。
- 所有文件不平等对待：避免学习那些在低级别中也很短暂的文件
- workload和数据感知：了解树的某些部分比其他部分更有益。

## 3.SIGMOD 2024：temporal json keyword search
**idea**