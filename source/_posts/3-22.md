---
title: 3.21,3.22,3.23
date: 2025-03-23 19:27:53
tags:
- 论文
---
 <!-- more -->
读论文。
## 1.PVLDB：memformer

**idea**：分块循环图学习，全局attention,交替记忆增强器
**knowledge**：
1. 正则化：**防止过拟合！** 显式正则化/隐式正则化，机器学习基础内容，回顾一下。
显：
- L1（lasso）正则化：向损失函数添加权重**绝对值之和**作为惩罚，迫使部分权重趋近于0。
- L2（ridge）正则化：向损失函数添加权重**平方之和**作为惩罚，限制权重幅度使其平滑。
- Dropout：训练时，关闭部分神经元（概率p），迫使网络学习冗余表示。测试时激活所有，权重乘（1-p）保持期望值。
- 弹性网络：L1L2搞在一起。
隐：
- 数据增强：训练数据加噪/旋转，增加数据多样性。
- 早停：验证集性能不再提升时终止训练，防止过拟合。
- 批量归一化：每层输入标准化，缓解内部协变量偏移。

2. 子序列建模：
- 通道独立/线性模型：每个变量单独并行处理，隐式参数共享/处理后数据整合。
- transformer类：注意力机制，异常值敏感。

3. 交替记忆增强
- 局部增强器：局部动态特征，细粒度关联建模。
- 全局增强器：全局模式，增强抗干扰。
- 交替训练机制。

***

## 2.OSDI 2020：learned index 4 lsm
**idea**：LSM树结合学习索引，贪婪线性回归学习。
**knowledge**
1. 学习索引
- 基于ml.
- 查询一个key时，系统使用该索引/该函数预测出查询key对应位置。
- 不直接构建数据结构节省空间，提升查找性能节省时间。
- 现有大多数在B树，所以用在lsm树上。

2. 理论“矛盾”
- 学习索引主要针对only-read,lsm针对write.
- 然而*insight*在于，虽然write改变了LSM,但树大多数部分不可变。——> 学习一个预测KV位置的值的函数只需要完成一次，只要**不可变的数据还在**就能使用它。

3. guideline after [Wisckey](https://zhuanlan.zhihu.com/p/486811231)
- 学习LSM中，稳定的低级别有效；查找先搜索更高级别，学习更高级别也有好处。
- 所有文件不平等对待：避免学习那些在低级别中也很短暂的文件
- workload和数据感知：了解树的某些部分比其他部分更有益。

***

## 3.SIGMOD 2024：temporal json keyword search
**idea**:时态数据模型，搜索分类，时变SLCA，
**knowledge**
1. 时态数据库
- 相对于传统数据库（被称为快照数据库，只存放当前状态），包含时态信息——时间区间/相对时间/时刻。
- 两个问题：**事件历史性问题**，**元事件时态信息**。
- 分类：快照。回滚——事务周期。历史——被管理对象的生命周期称为有效时间。双时态——既能管理对象历史，又能管理数据库本身历史，四维<元组，属性，事务时间，有效时间>。

2. PBN杜威顺序
- 分层数据实例节点编号方案，基于前缀。
- $p.k$，p（前缀）是父节点编号，k表示他是文档中第k个兄弟节点。

3. SLCA和LCA
- LCA：最低公共祖先。所有公共祖先**最深的**。
- SLCA：在属性结构中查找满足特定条件的**最小公共祖先节点**计算方法。所有符合条件的LCA中，**包含最小子树**。
- SLCA计算方法：基于区间编码，动态规划，dfs+栈。

***

## 4.SIGMOD 2024：AdapTraj 多智能体轨迹预测
**idea**：现有方法在多智能体轨迹预测中泛化能力差。——>adapTraj，提取focal和neighbor智能体的**域不变特征**和**域特定特征**。——>新的因果建模预测方法
**knowledge**：
1. 域domain，理解为区域
- 域不变特征：不同领域数据稳定，跨领域共和国想因果关系。
- 域特定特征：只在特定领域内存在的特征。

2. 负迁移（迁移学习中）
- 原领域（src）的知识迁移到目标领域（tar），模型在目标领域性能下降。

3. 因果预测
- 区分**因果效应**和**统计相关性**。
- DAG图表示因果关系。
- robust和explaining