---
title: ml漫游（1）
date: 2025-03-26 14:19:48
tags:
- 课内
---
 <!-- more -->

## leadin： 初探

重点讨论服务预测任务的ml技术，支撑决策任务的是强化学习R（reinforce）L。

学习：基于数据提升既定指标分数。
机器学习：算法通过非显式编程（开发者不需要自己可以完成任务）在某个任务上，通过经验数据来提升性能指标，使用三元组<任务，指标，数据>。

#### 主流分类方式

- 无监督学习：数据没有标签，所有数据维度同等重要。只关心数据中隐含的模式。往往使用概率分布模型p(x)建模数据分布。整个数据集对数似然作为无监督学习最大化目标。

- 有监督学习：每个数据实例由**特征和标签**组成。模型任务是根据特征预测标签。性能使用损失函数L(y,f(x))来定义，衡量预测偏差。

- 强化学习：关注决策问题，寻找更好决策策略，优化目标是策略带来的累计回报。

#### 建模方式分类
- 参数化模型：预先定义model family,模型使用具体的参数向量唯一确定，参数个数不变（占用内存/显存不变）。求最优可以借助针对参数的梯度来完成。

- 非参数化模型：直接在模型空间寻找模型实例。模型参数并非一一对应，数据量不同会导致模型中参数量不同。一般来说，数据量越大，参数量越多。

**泛化能力**：在没见过的数据上的性能。

**归纳偏置**：模型对问题的先验假设。（模型的某种bias）。

***

## KNN

根据样本x**最近的K个样本**中出现次数最多的类别，确定样本x的类别。

- 可用于回归任务：预测样本x对应的实数y：K个相邻的样本点对应的实数y加权平均。

## 线性回归

假设输入输出呈线性关系。

$f_\theta (x)=\theta^Tx=\theta_1x_1+...+\theta_dx_d$（常数项表达为x0=1）

- 损失函数设计：最小化损失函数。均方误差（MSE），y与f(x)偏差二次速度增长。不用花大精力消除最后一点损失。

#### 解析方法

输入的向量（数据实例特征向量）和标签组合成矩阵
——>以向量平方作为损失函数：所有样本平方误差的和。

#### 梯度下降GD

bg：解析解耗费时间空间or无法求解。

使用数值模拟求近似解。按照梯度方向一直前进到局部最小值，对于凸函数就是全局最小。

参数$\theta$，损失$J\theta$。梯度下降：$\theta \gets \theta -\eta \nabla _\theta J(\theta)$

$\nabla$是更新步长，称为学习率。

如果函数非凸，可能陷入局部最小值。

当样本量很大是，计算矩阵向量相乘仍然很耗时，矩阵存储问题也没有解决。可以每次只随机选一个样本计算梯度。称为**随机梯度下降法（SGD）**

SGD不稳定，可能单样本梯度方向与所有样本算出的真正梯度方向不同。为了平衡稳定性和时间，使用**小批量梯度下降（MBGD）**，参数B：batch。

B：1,SGD；N，GD。将样本分批，每次选一批计算梯度。

## 机器学习基本思想

#### 欠拟合与过拟合

- 数据模式：数据背后规律。
- 数据噪声：数据点对于数据集模式的偏差。


**欠拟合**：无法拟合重要模式，精度低。
**过拟合**：过度拟合到训练数据中的噪声，预测有很大偏差。

本质上，过拟合是模型参数过于复杂引起。所以引入正则化约束。

#### 正则化约束

理论上，如果数据集数据个数N小于数据特征数d。
此时线性规划模型解析解：$\theta =(X^TX)^{-1}X^Ty$无法计算。
加入正则项后使得解析解始终存在。

- L0范数：向量中非0元素个数，使向量中非0元素尽可能少。
- L1范数：LASSO回归，一次，正方形区域。
- L2范数：岭回归，二次，圆形区域。

两种思想：
1. 机器先计算样本之间相似偶读，再用统计的思想利用相似度给出模型对样本的预测。
2. 为了让相似度计算更加符合任务要求，可以先用特征映射把样本中有用信息提取出来，降低后续特征利用难度。

#### 参数与超参数

- 参数：训练中不断优化的变量。
- 超参数：预设好的，不再变化。（调参也是调这个）。尽量减少超参数数量。

#### 数据集划分与交叉验证

1. 先划分训练集和测试集，防止测试集信息泄露。
2. 训练集与真实的测试集中数据分布规律相同。

对两方都适用：迁移学习。

交叉验证：分成k份，k轮训练，每次一份验证其他作为训练。最终在k次得到的误差的平均值作为最终误差。一般k取[5,10]。

*** 

## 逻辑斯蒂回归

参数化模型处理分类问题。

主要考虑二分类（多分类可以多次使用二分类获得）

#### 线性模型

将样本x的类别y看作有0和1两种取值的随机变量，判断y=0或1的概率（给出概率分布），将x归为概率较大的一类即可。

将$\theta ^Tx$取值范围R映射到[0,1]使用逻辑斯蒂函数（sigmoid）。在其他页有记录。

#### 最大似然估计

使用最大似然估计（MLE）来优化模型，即寻找逻辑斯蒂回归参数$\theta$使得模型预测正确概率最大。概率论学过不赘述。

对于多分类任务，使用柔性最大值函数（softmax）将$\theta ^Tx$映射到多分类概率。

#### 分类问题评价指标

阈值：y为正类是否满足？

混淆矩阵（confusion matrix）判断分类结果。

- 真阳性TP：阳->阳
- 真阴性TN：阴->阴
- 假阳性FP：阴->阳
- 假阴性FN：阳->阴

精度（accuracy）：$\frac{TP+TN}{TP+FP+TN+FN}$

真阳性率/查全率/召回率（recall）：$Rec=\frac{TP}{TP+FN}$

查准率（precision）：$Prec=\frac{TP}{TP+FP}$

F1分数：精确率和召回率调和平均值。单方面增大或减小精准率和召回率无法使F1分数增高，必须让两者平衡。如果两者重要程度不同，扩展为F-Beta分数。

整体表现：变化趋势相同的假阳性率和真阳性率。
$FPR=\frac{FP}{FP+TN}$
$TPR=\frac{TP}{TP+FN}$
TPR随FPR变化：受试者操作特征曲线（ROC）。
曲线下面积（AUC）。随机预测时是0.5。与阈值无关，只和模型对正负类预测有关。

希望FPR尽可能低，TPR高。


最理想：FPR=0,TPR=1。

多分类任务中，
对所有类别的F1做平均，得到最终指标称为macro-F1分数。
根据每个类别样本数量对F1做加权平均，最终得到micro-F1分数


#### 交叉熵与最大似然估计

随机事件发生的概率越小，提供的信息量越大。设事件$X_i$发生概率为$P(X_i)$,提供的信息为$I(X_i)=-logP(X_i)$

如果分布中，某事件发生概率很大，预测难度就较低；如果各个事件发生概率接近，分布的不确定性更大。
衡量无序程度**熵（entropy）**
$$H(p)=-\sum_{i=1}^{n}P(X_i)logP(X_i)$$

当某个事件发生概率为1,其他为0,分布的熵最小，H=0；如果都等概率，熵最大，H=logn。

如果随机变量X存在两个概率分布p(x)和q(x)，用**相对熵（KL散度）**衡量两个分布距离。**交叉熵H(p,q)**主要用于衡量两个概率分布之间的差异性。
KL散度无法通过模型优化，只需要最小化交叉熵H(p,q)。

*根据数学推导，最大化似然函数和最小化交叉熵等价。*

***

## 双线性模型

一个二元函数，对其中两个变量保持线性关系（即固定一个，另一个线性）。

#### 矩阵分解MF

推荐系统评分预测（rating predict）的常用模型。以用户对电影评分为例。

电影特征库，每个电影用一个特征向量表示，每维代表一个特征，值代表电影具有这一特征的程度。
用户画像库，用户偏好的类型特征与偏好程度。

我们分解的矩阵是某种交互结果的隐变量，并非对应真正特征。用两个矩阵成绩乘积出用户对电影评分。


实际上，通常获得的是打分结果，并且由于一个用户只会对有限一部分电影打分，打分结果矩阵很稀疏。
所以使用**矩阵分解**完成任务。

所以损失函数为**L(用户偏好和电影特征的内积，真实打分)**
一般用MSE作为损失和L2正则化约束。

#### 因子分解机FM

希望通过物品特征和用户点击的历史记录，预测点击其他物品的概率（点击率，CTR）
点击/不点击是二分类问题，可以用逻辑斯蒂回归实现。但一般的线性参数化假设，输入不同特征$x_i$和$x_j$之间没有运算。但在现实中很可能不独立，所以将不同特征的联系也考虑进来（交叉相乘配权重）。

面临稀疏的挑战：**独热编码。**
每维度都对应特征的一种取值，样本具有的特征所在维度1其他0。如果有多个特征，就把每个拼接起来，形成**多域独热编码**。

使用因子分解机应对这个问题，将权重矩阵分解为*低秩隐向量*，参数规模由$O(d^2)$降低至$O(kd)$,k是隐向量维度。（本质上是数学变换）。


























