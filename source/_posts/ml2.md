---
title: ml漫游（2）
date: 2025-03-27 08:53:52
tags:
- 课内
---
 <!-- more -->

## 神经网络与多层感知机

#### 人工神经网络NN

最基本单位是神经元。仿照生物神经元的结构和行为方式。互相连接构成有向图，每个神经元是一个节点，神经元之间连接就作为有向边。
表达式：$z_j=\sum_{i=1}^{N_j}w_{ji}o_i$
其中z为信号总和，o为神经元发送的信号，w为权重。

#### 感知机

每条边权重需要人为制定，当规模较大时，很难先验的用数学方法算出合适的权重。

相比最初的神经网络，增加了bias(+b)，通过激活函数（模拟神经元兴奋或抑制），得到最终输出。

可以自动调整：**负反馈机制**。返回预测值与真实的差值，（学习率$\eta$）调整权重和偏置项。

但只能解决线性问题，碰到xor问题时出现瓶颈

#### 隐含层与多层感知机MLP

增加网络层数，将一个感知机的输出作为输入连接到下一个感知机，起到超越线性划分的效果。

组合多个单层感知机时，常常使用**前馈结构**：神经元分不同的层，每层只和其前后相邻层的神经元连接，层内/间隔一层以上的神经元之间没有连接。

结构分为输入层->隐含层->输出层。

*层数指权重的层数，即边的层数，神经元上不携带权重*。

更广的拟合能力：非线性激活函数。（相当于提升了数据的维度）

- 逻辑斯蒂函数：x映射到(0,1)。直观上0对应京西，1对应兴奋。

- 双曲正切：x映射到(-1,1)

- 线性整流单元：小于0的变0，大于0的不变。

普适逼近定理：任意$R^n$上的连续函数，都可以用大小合适的MLP来拟合。

连续的线性变换等价于一个线性变换。非线性变换将数据提升到更高维度中，低维中线性不可分的，在高维空间中就变得线性可分。

#### 反向传播

设最小化目标$J(x)$，依然需要计算网络中各个参数$\nabla J$。对于前馈网络，每层计算依次进行，链式法则。
















