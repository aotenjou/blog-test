---
title: ml漫游（2）
date: 2025-03-27 08:53:52
tags:
- 课内
---
 <!-- more -->

## 神经网络与多层感知机

#### 人工神经网络NN

最基本单位是神经元。仿照生物神经元的结构和行为方式。互相连接构成有向图，每个神经元是一个节点，神经元之间连接就作为有向边。
表达式：$z_j=\sum_{i=1}^{N_j}w_{ji}o_i$
其中z为信号总和，o为神经元发送的信号，w为权重。

#### 感知机

每条边权重需要人为制定，当规模较大时，很难先验的用数学方法算出合适的权重。

相比最初的神经网络，增加了bias(+b)，通过激活函数（模拟神经元兴奋或抑制），得到最终输出。

可以自动调整：**负反馈机制**。返回预测值与真实的差值，（学习率$\eta$）调整权重和偏置项。

但只能解决线性问题，碰到xor问题时出现瓶颈

#### 隐含层与多层感知机MLP

增加网络层数，将一个感知机的输出作为输入连接到下一个感知机，起到超越线性划分的效果。

组合多个单层感知机时，常常使用**前馈结构**：神经元分不同的层，每层只和其前后相邻层的神经元连接，层内/间隔一层以上的神经元之间没有连接。

结构分为输入层->隐含层->输出层。

*层数指权重的层数，即边的层数，神经元上不携带权重*。

更广的拟合能力：非线性激活函数。（相当于提升了数据的维度）

- 逻辑斯蒂函数：x映射到(0,1)。直观上0对应京西，1对应兴奋。

- 双曲正切：x映射到(-1,1)

- 线性整流单元：小于0的变0，大于0的不变。

普适逼近定理：任意$R^n$上的连续函数，都可以用大小合适的MLP来拟合。

连续的线性变换等价于一个线性变换。非线性变换将数据提升到更高维度中，低维中线性不可分的，在高维空间中就变得线性可分。

#### 反向传播

设最小化目标$J(x)$，依然需要计算网络中各个参数$\nabla J$。对于前馈网络，每层计算依次进行，**链式法则(数学基础）**。

每一层的梯度由两部分组成。一部分是当前的激活函数计算出的梯度，另一部分是后一层回传的梯度。


***

## 卷积神经网络CNN

提取高维特征的运算和网络结构。

#### 卷积

系统f对信号g的响应为两者重叠部分相乘后图像的面积。结果仍然是相对t的函数。卷积就是在求t时刻$g(t-\tau)$按$f(\tau)$加权的平均值。因此我们可以认为，卷积的本质是计算某种特殊的加权平均。

$(f*g)(t)=\int_{-\infty }^{\infty} f(\tau )g(t-\tau)d\tau $

神经网络中，我们可以将f设置为可以训练的参数，通过梯度反向传播的方式进行训练，自动调整其权重值。

与MLP中的线性变换不同，主要由卷积运算构成的神经网络就称为卷积神经网络（CNN），CNN中进行卷积运算的层称为卷积层，层中的权重f称为卷积核。

有时候，我们希望输出图像能保持和输入图像同样的大小，因此会对输入图像的周围进行填充（padding），以此抵消卷积对图像尺寸的影响。填充操作会在输入图像四周补上数层额外的像素（常数/边界扩展）

卷积运算可以对一定范围内的图像进行特征提取，其提取范围就是卷积核的大小（宽*高）。因此，卷积核的大小又称为卷积核的感受野（receptive field）。

考虑到图像中通常会有大量相邻的相似像素，另一方面，小卷积核对局部的信息可能过于敏感。
因此，会对图像进行池化（pooling）操作。池化是一种降采样（downsampling）操作，即用一个代表像素来替代一片区域内的所有像素，从而降低整体的复杂度。
常用的池化有平均池化和最大池化两种。平均池化（average pooling）是用区域内所有像素的平均值来代表区域，最大池化（max pooling）是用所有像素的最大值来代表区域。

不同的卷积核表示的是图像上同一个位置的不同特征，与图像的色彩通道含义类似，我们将卷积核的数量也称为通道（channel）。
对于多个通道的输入，我们可以同样使用多通道卷积核来进行计算，得到多通道的输出。

除此之外，我们还需要加入丢弃层（dropout）。
丢弃层会在每次前向传播时，随机把输入中一定比例的神经元遮盖住，使它们对后面的输出不产生梯度回传。
对于模型来说，相当于这些神经元暂时“不存在”，从而降低了模型的复杂度，可以缓解模型的过拟合问题。

#### VGG网

即通过反复堆叠基础的模块来构建网络，而无须像AlexNet一样为每一层都调整卷积核和池化的大小。

***

## 循环神经网络RNN

![](https://pic1.imgdb.cn/item/67e4c6160ba3d5a1d7e48e1e.png)

![](https://pic1.imgdb.cn/item/67e4c62f0ba3d5a1d7e48e75.png)

本质上是多层MLP中间变量的流水线，后面的纳入前面的结果。

可能出现**梯度消失和梯度爆炸**。

我们可以将网络中关联起相邻两步的$f_h$和$W_h$扩展成一个小的网络，通过设计其结构来达到稳定梯度的目的。


#### 门控制网络GRU

![](https://pic1.imgdb.cn/item/67e4c9600ba3d5a1d7e49585.png)

门的输出$h_t$由新的输入$x_t$和之前的输出$h_{t-1}$决定。其中，是$z_t$更新单元，$r_t$是重置单元。

利用与**重置单元的Hadamard积来选择性遗忘。利用更新单元来设置新的输入xt对输出的影响。调整两个单元避免梯度消失和梯度爆炸。**

***

## 支持向量机SVM

非参数化模型。
对于数据分类，使分隔直线对数据点的扰动或噪声的鲁棒性更强，我们希望从这无数个可以分隔两个点集的超平面中，挑选出与任意一点间隔（margin）的最小值最大的平面。

转换为二次规划的求解。$\alpha$是待求解参数。

松弛变量$\xi _i$解决线性不可分：允许某个样本点的类别与超平面给出的分类不同。对这些分类错误的样本点，我们也需要进行惩罚。
因此，我们在优化的目标函数上加入值$\xi_i$本身作为惩罚项（乘以C惩罚系数）

与超平面间隔最小的向量称为支持向量（supporting vector).

#### 序列最小优化SMO

核心思想：同时优化所有$\alpha$比较困难，由于对$\aplha$有等式约束，每次选两个不同的$\alpha_i,\alpha_j$进行优化，保持等式约束成立，固定其他参数。再选其他两个。
知道目标函数只收敛位置。

#### 核函数

SVM引入非线性特征映射。$x\rightarrow \phi(x)$。

常用的核函数：

- 内积核：$K(x,y)=x^Ty$
- 简单多项式核：$K(x,y)=(x^Ty)^d$
- 高斯核：$K(x,y)=exp(-\frac{||x-y||^2}{2\sigma ^2})$，又称径向基函数(RBF)核。
- 余弦相似度核：$K(x,y)=\frac{x^Ty}{||x||\cdot||y||} $，计算向量xy间的夹角。
- sigmoid核：$K(x,y)=tanh(\beta x^Ty+c)$，可以证明，带sigmoid核的SVM等价两层的多层感知机。

***

## 决策树

非参数化模型。
每个内部节点包含一个关于x的表达式，每个分支对应表达式一种取值。叶节点是决策树输出。

主要考虑**表达式的选择**。

使用概率的熵作为信息的度量。引入额外信息时，不确定度会减小，熵会降低。

数据集中样本类别是X,引入分类特征是Y.数据集已知，X分布和熵也已知。需要考虑加入特征Y后的信息增益。计算交叉熵。

信息增益率：获得的信息量 和 Y关于X复杂度 的比值。

#### ID3与C4.5

**ID3**

- 每次选择信息增益最大的特征进行分类，对产生的子节点递归特征选取和分裂，直到所有节点上数据属于同一类。

存在两个问题：

1. 如果划分特征本身很复杂，可能出现复杂度转移（到多个分支选择），只对当前数据有效（过拟合）
选取标准由信息增益改为信息增益率，得到C4.5。

2. ID3会将所有数据分类完才停止，如果包含很多相似数据，ID3构造会出现很长的分支，增加算法复杂度。
仿照正则化约束，引入决策树T的代价函数C(T).


#### CART决策树

回归问题。

同一叶节点内样本标签值的平均作为模型对该区域的预测。

CART使用*误差的平方和*作为回归问题下寻找最优特征的标准。

*每次划分选择使两个区域的平方误差和最小的特征j和区域s.*

对于阈值s,把当前节点上样本按特征值由小到大排序，记为$j_1,...,j_r$，并让s初始化为最中间的值，再不断通过比较两边损失的大小来移动s.

即让误差尽量平衡。

也可用来构建分类树。划分指标变为计算基尼不纯度：$Gini(p)=1-\sum{k=1}^{K}p(k)^2$.
所有概率值相等时，函数取得最大值1-1/K.X的值完全确定时，函数取得最小值0。

***

## K均值聚类 K-means

无监督学习算法。

#### 原理

数据集x 特征维数都是d.最终聚簇个数K提前制定。一开始不能确定每一类的中心(centroid)。

初始时随机选择K个点作为中心。选取中心后，用最简单方式，把数据集的点归入中心代表的类中。
减小类中的点到中心点的距离，将数据集中所有点到其对应中心距离之和作为损失函数。用欧式距离度量，再对中心点求偏导，使其为0，得到最优点。

#### kmeans++

k均值算法对初始选择的聚类中心敏感，且容易收敛到最小值。

kmeans++从所有点中随机选一个，当第一个聚类中心点。我们希望初始中心点散开，因此选择接下来的中心点时，**会把当前中心点纳入考量**。使各个中心点距离尽可能大。
样本x被选为中心点概率与其到当前中心点距离的平方成正比。

***

## 主成分分析PCA

数据降维：从高维数据中提取出具有代表性、最大限度保留数据本身信息的特征。主成分分析是最经典的数据降维算法。

#### 主成分与方差

设原始数据维度d,我们希望找到某种变换，将数据维度减小到k,k << d，变换后数据不同维度线性无关。*这些维度称为主成分*。

要求特征之间相互独立：协方差为0.所以，每个主成分计算时与其他主成分*正交*，直到选到预先设定的k个。

计算过程：**寻找方差最大的方向作为主成分**。即，向某个方向投影，数据最容易区分。

#### 特征分解计算PCA

数学推导。思路是：寻找求解PCA最大特征之及其对应特征向量过程。由于包含的协方差矩阵半正定，其所有特征向量正交。

***

## 概率图模型

建模数据中样本关于各个特征的分布。

设法寻找特征的相关性关系：分布转化为条件概率。比如$t\rightarrow c$表示c依赖t。

这样按依赖关系列出所有概率图，每个特征作为一个定点，依赖关系作为边。
通过概率图体现的不同特征的关系，推断出数据概率分布。

有依赖关系构成的概率图是有向图，称**贝叶斯网络**。

无向图称**马尔科夫网络**。

#### 贝叶斯网络

又称信念网络，与概率中贝叶斯推断有很大关联。
*尾对尾*的依赖。**条件独立**是概率图模型中最重要的基本概念之一。

独立用$\upmodels$表示。

后面用得少，不记录了。





