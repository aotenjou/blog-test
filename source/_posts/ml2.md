---
title: ml漫游（2）
date: 2025-03-27 08:53:52
tags:
- 课内
---
 <!-- more -->

## 神经网络与多层感知机

#### 人工神经网络NN

最基本单位是神经元。仿照生物神经元的结构和行为方式。互相连接构成有向图，每个神经元是一个节点，神经元之间连接就作为有向边。
表达式：$z_j=\sum_{i=1}^{N_j}w_{ji}o_i$
其中z为信号总和，o为神经元发送的信号，w为权重。

#### 感知机

每条边权重需要人为制定，当规模较大时，很难先验的用数学方法算出合适的权重。

相比最初的神经网络，增加了bias(+b)，通过激活函数（模拟神经元兴奋或抑制），得到最终输出。

可以自动调整：**负反馈机制**。返回预测值与真实的差值，（学习率$\eta$）调整权重和偏置项。

但只能解决线性问题，碰到xor问题时出现瓶颈

#### 隐含层与多层感知机MLP

增加网络层数，将一个感知机的输出作为输入连接到下一个感知机，起到超越线性划分的效果。

组合多个单层感知机时，常常使用**前馈结构**：神经元分不同的层，每层只和其前后相邻层的神经元连接，层内/间隔一层以上的神经元之间没有连接。

结构分为输入层->隐含层->输出层。

*层数指权重的层数，即边的层数，神经元上不携带权重*。

更广的拟合能力：非线性激活函数。（相当于提升了数据的维度）

- 逻辑斯蒂函数：x映射到(0,1)。直观上0对应京西，1对应兴奋。

- 双曲正切：x映射到(-1,1)

- 线性整流单元：小于0的变0，大于0的不变。

普适逼近定理：任意$R^n$上的连续函数，都可以用大小合适的MLP来拟合。

连续的线性变换等价于一个线性变换。非线性变换将数据提升到更高维度中，低维中线性不可分的，在高维空间中就变得线性可分。

#### 反向传播

设最小化目标$J(x)$，依然需要计算网络中各个参数$\nabla J$。对于前馈网络，每层计算依次进行，**链式法则(数学基础）**。

每一层的梯度由两部分组成。一部分是当前的激活函数计算出的梯度，另一部分是后一层回传的梯度。


***

## 卷积神经网络CNN

提取高维特征的运算和网络结构。

#### 卷积

系统f对信号g的响应为两者重叠部分相乘后图像的面积。结果仍然是相对t的函数。卷积就是在求t时刻$g(t-\tau)$按$f(\tau)$加权的平均值。因此我们可以认为，卷积的本质是计算某种特殊的加权平均。

$(f*g)(t)=\int_{-\infty }^{\infty} f(\tau )g(t-\tau)d\tau $

神经网络中，我们可以将f设置为可以训练的参数，通过梯度反向传播的方式进行训练，自动调整其权重值。

与MLP中的线性变换不同，主要由卷积运算构成的神经网络就称为卷积神经网络（CNN），CNN中进行卷积运算的层称为卷积层，层中的权重f称为卷积核。

有时候，我们希望输出图像能保持和输入图像同样的大小，因此会对输入图像的周围进行填充（padding），以此抵消卷积对图像尺寸的影响。填充操作会在输入图像四周补上数层额外的像素（常数/边界扩展）

卷积运算可以对一定范围内的图像进行特征提取，其提取范围就是卷积核的大小（宽*高）。因此，卷积核的大小又称为卷积核的感受野（receptive field）。

考虑到图像中通常会有大量相邻的相似像素，另一方面，小卷积核对局部的信息可能过于敏感。
因此，会对图像进行池化（pooling）操作。池化是一种降采样（downsampling）操作，即用一个代表像素来替代一片区域内的所有像素，从而降低整体的复杂度。
常用的池化有平均池化和最大池化两种。平均池化（average pooling）是用区域内所有像素的平均值来代表区域，最大池化（max pooling）是用所有像素的最大值来代表区域。

不同的卷积核表示的是图像上同一个位置的不同特征，与图像的色彩通道含义类似，我们将卷积核的数量也称为通道（channel）。
对于多个通道的输入，我们可以同样使用多通道卷积核来进行计算，得到多通道的输出。

除此之外，我们还需要加入丢弃层（dropout）。
丢弃层会在每次前向传播时，随机把输入中一定比例的神经元遮盖住，使它们对后面的输出不产生梯度回传。
对于模型来说，相当于这些神经元暂时“不存在”，从而降低了模型的复杂度，可以缓解模型的过拟合问题。

#### VGG网

即通过反复堆叠基础的模块来构建网络，而无须像AlexNet一样为每一层都调整卷积核和池化的大小。

***

## 循环神经网络RNN

![](https://pic1.imgdb.cn/item/67e4c6160ba3d5a1d7e48e1e.png)

![](https://pic1.imgdb.cn/item/67e4c62f0ba3d5a1d7e48e75.png)

本质上是多层MLP中间变量的流水线，后面的纳入前面的结果。

可能出现**梯度消失和梯度爆炸**。

我们可以将网络中关联起相邻两步的$f_h$和$W_h$扩展成一个小的网络，通过设计其结构来达到稳定梯度的目的。


#### 门控制网络GRU

![](https://pic1.imgdb.cn/item/67e4c9600ba3d5a1d7e49585.png)

门的输出$h_t$由新的输入$x_t$和之前的输出$h_{t-1}$决定。其中，是$z_t$更新单元，$r_t$是重置单元。

利用与**重置单元的Hadamard积来选择性遗忘。利用更新单元来设置新的输入xt对输出的影响。调整两个单元避免梯度消失和梯度爆炸。**

***

## 支持向量机SVM

非参数化模型。
对于数据分类，使分隔直线对数据点的扰动或噪声的鲁棒性更强，我们希望从这无数个可以分隔两个点集的超平面中，挑选出与任意一点间隔（margin）的最小值最大的平面。

松弛变量$\xi _i$解决线性不可分：允许某个样本点的类别与超平面给出的分类不同。对这些分类错误的样本点，我们也需要进行惩罚。
因此，我们在优化的目标函数上加入值$\xi_i$本身作为惩罚项（乘以C惩罚系数）

与超平面间隔最小的向量称为支持向量（supporting vector).





